# PDF Outline Extractor - Challenge 1A

This folder contains the optimized ML-enhanced PDF outline extraction system for Challenge 1A.

## üéØ Performance Results
- **ML-Enhanced F1 Score: 0.790**
- **Improved Extractor F1 Score: 0.433** 
- **ML vs Improved: +82.6%**

## üìÅ File Structure

### Core Components
- **`ml_enhanced_extractor.py`**: The main entry point for the ML-enhanced outline extraction. It integrates the feature extractor and the trained model to provide the final outline.
- **`improved_extractor.py`**: A purely rule-based extractor, which serves as a baseline for comparison.
- **`pdf_feature_extractor.py`**: A dedicated module for extracting a rich set of features from PDF documents, which are then used by the ML model.

### Machine Learning
- **`Training_data_rf_model.pkl`**: The pre-trained Random Forest classifier that powers the ML-enhanced extractor.
- **`model_trainer.py`**: The script used to train the Random Forest model. It includes hyperparameter tuning using GridSearchCV to find the best model configuration.
- **`generate_training_data.py`**: A utility script to create the training dataset (`Training_data.xlsx`) from a directory of PDFs and their corresponding ground truth JSON files.

### Testing and Evaluation
- **`test_ml_enhanced.py`**: A comprehensive test suite to evaluate and compare the performance of the ML-enhanced and improved extractors. It calculates precision, recall, and F1 scores and generates a detailed report.
- **`pdfs/`**: A directory containing a variety of PDF files used for testing and evaluation.
- **`test_outputs_.../`**: Directories where the output of the test script is saved, including the extracted outlines in JSON format and a summary report.


## üöÄ Usage

To evaluate the performance of the extractors, simply run the test script:

```bash
python test_ml_enhanced.py
```

This will:
1.  Load the pre-trained ML model.
2.  Run both the ML-enhanced and the improved rule-based extractors on a set of test PDFs.
3.  Calculate and display a detailed performance comparison, including F1 scores.
4.  Save the extracted outlines and a summary report in a new `test_outputs_...` directory.

To retrain the model:
1.  Ensure you have ground truth JSON files in the `Challenge_1a/Datasets/Output.json` directory.
2.  Run the training data generator: `python generate_training_data.py pdfs/ ../Challenge_1a/Datasets/Output.json/ Training_data.xlsx`
3.  Run the model trainer: `python model_trainer.py Training_data.xlsx`

```python
from ml_enhanced_extractor import MLEnhancedPDFExtractor

# With ML model (best performance)
extractor = MLEnhancedPDFExtractor(model_path='enhanced_model.pkl')
result = extractor.extract_outline_json('document.pdf')

# Rule-based only (Challenge 1A compliance)
extractor = MLEnhancedPDFExtractor(use_ml=False)
result = extractor.extract_outline_json('document.pdf')
```

## üèÜ Key Features
- ‚úÖ **No hardcoding** - Fully adaptive to document characteristics
- ‚úÖ **ML-enhanced** - Random Forest classifier with intelligent feature engineering
- ‚úÖ **Challenge 1A compliant** - Rule-based fallback when ML unavailable
- ‚úÖ **Ensemble approach** - Combines ML confidence with rule-based scoring
- ‚úÖ **Advanced text processing** - Handles fragmented text and complex layouts
- ‚úÖ **Document-aware filtering** - Adaptive to forms, academic papers, manuals, etc.

## üìä Performance by Document Type
- **Forms (E0CCG5S239)**: ML Perfect F1 (1.000) vs Improved (0.000)
- **Academic Papers (STEMPathwaysFlyer)**: Both approaches F1 (0.500) 
- **Technical Documents (E0CCG5S312)**: Improved F1 (0.593) vs ML (0.462)
- **Government Documents (E0H1CM114)**: Improved F1 (0.610) vs ML (0.250)
- **Invitations (TOPJUMP)**: Improved F1 (0.667) vs ML (0.400)

## üîß Technical Details
- **Algorithm**: Random Forest + Rule-based ensemble
- **Features**: 12 engineered features (font size, formatting, structure, position, etc.)
- **Training**: 54 samples with class balancing
- **Output**: Challenge 1A JSON format with 0-based page numbers
- **Dependencies**: PyMuPDF, scikit-learn, pandas (with graceful fallback)
